#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ‡ç‰‡ç­–ç•¥å¯¹æ¯”æµ‹è¯•è„šæœ¬
å±•ç¤º6ç§ä¸åŒåˆ‡ç‰‡ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”
"""

import re
import os
from openai import OpenAI

# 1. å›ºå®šé•¿åº¦åˆ‡ç‰‡
def improved_fixed_length_chunking(text, chunk_size=512, overlap=50):
    """å›ºå®šé•¿åº¦åˆ‡ç‰‡ - åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†
        if end < len(text):
            # å¯»æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
            for i in range(end, max(start, end - 100), -1):
                if text[i] in '.!?ã€‚ï¼ï¼Ÿ':
                    end = i + 1
                    break
        
        chunk = text[start:end]
        
        if len(chunk.strip()) > 0:
            chunks.append(chunk.strip())
        
        start = end - overlap
    
    return chunks

# 2. å¥å­è¾¹ç•Œåˆ‡ç‰‡
def semantic_chunking(text, max_chunk_size=512):
    """åŸºäºå¥å­è¾¹ç•Œçš„åˆ‡ç‰‡ - æŒ‰å¥å­åˆ†å‰²"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ\n]+', text)
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # å¦‚æœå½“å‰å¥å­åŠ å…¥åè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä¿å­˜å½“å‰å—
        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

# 3. LLMè¯­ä¹‰åˆ‡ç‰‡ï¼ˆLLMï¼‰
def advanced_semantic_chunking_with_llm(text, max_chunk_size=512):
    """ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ‡ç‰‡"""
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("è­¦å‘Š: æœªè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯­ä¹‰åˆ‡ç‰‡")
        return semantic_chunking(text, max_chunk_size)
    
    client = OpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    
    prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æœ¬æŒ‰ç…§è¯­ä¹‰å®Œæ•´æ€§è¿›è¡Œåˆ‡ç‰‡ï¼Œæ¯ä¸ªåˆ‡ç‰‡ä¸è¶…è¿‡{max_chunk_size}å­—ç¬¦ã€‚
è¦æ±‚ï¼š
1. ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
2. åœ¨è‡ªç„¶çš„åˆ†å‰²ç‚¹åˆ‡åˆ†
3. è¿”å›JSONæ ¼å¼çš„åˆ‡ç‰‡åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "chunks": [
    "ç¬¬ä¸€ä¸ªåˆ‡ç‰‡å†…å®¹",
    "ç¬¬äºŒä¸ªåˆ‡ç‰‡å†…å®¹",
    ...
  ]
}}

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ‡ç‰‡åˆ—è¡¨ï¼š
"""
    
    try:
        response = client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ‡ç‰‡åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡è®°ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        
        result = response.choices[0].message.content
        
        # æ¸…ç†ç»“æœï¼Œç§»é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°
        cleaned_result = result.strip()
        if cleaned_result.startswith('```'):
            cleaned_result = re.sub(r'^```(?:json)?\s*', '', cleaned_result)
        if cleaned_result.endswith('```'):
            cleaned_result = re.sub(r'\s*```$', '', cleaned_result)
        
        # è§£æJSONç»“æœ
        chunks_data = json.loads(cleaned_result)
        
        # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
        if "chunks" in chunks_data:
            return chunks_data["chunks"]
        elif "slice" in chunks_data:
            if isinstance(chunks_data, list):
                return [item.get("slice", "") for item in chunks_data if item.get("slice")]
            else:
                return [chunks_data["slice"]]
        else:
            if isinstance(chunks_data, list):
                return chunks_data
            else:
                return []
        
    except Exception as e:
        print(f"LLMåˆ‡ç‰‡å¤±è´¥: {e}")
        return semantic_chunking(text, max_chunk_size)

# 4. æ»‘åŠ¨çª—å£åˆ‡ç‰‡
def sliding_window_chunking(text, window_size=512, step_size=256):
    """æ»‘åŠ¨çª—å£åˆ‡ç‰‡"""
    chunks = []
    
    for i in range(0, len(text), step_size):
        chunk = text[i:i + window_size]
        
        if len(chunk.strip()) > 0:
            chunks.append(chunk.strip())
    
    return chunks

# 5. è‡ªé€‚åº”åˆ‡ç‰‡
def adaptive_chunking(text, target_size=512, tolerance=0.2):
    """è‡ªé€‚åº”åˆ‡ç‰‡ - æ ¹æ®å†…å®¹è‡ªé€‚åº”è°ƒæ•´"""
    chunks = []
    
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    current_chunk = ""
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
        
        # å¦‚æœå½“å‰æ®µè½åŠ å…¥åè¶…è¿‡ç›®æ ‡å¤§å°
        if len(current_chunk) + len(paragraph) > target_size * (1 + tolerance):
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += " " + paragraph if current_chunk else paragraph
    
    # å¤„ç†æœ€åä¸€ä¸ªå—
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

# 6. æ™ºèƒ½è‡ªé€‚åº”åˆ‡ç‰‡
def smart_adaptive_chunking(text, target_size=512, min_size=100, max_size=1000):
    """æ™ºèƒ½è‡ªé€‚åº”åˆ‡ç‰‡ - è€ƒè™‘è¯­ä¹‰å’Œé•¿åº¦"""
    chunks = []
    
    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ\n]+', text)
    
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°å—
        if (len(current_chunk) + len(sentence) > max_size and 
            len(current_chunk) >= min_size):
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        elif len(current_chunk) + len(sentence) > target_size and len(current_chunk) >= min_size:
            # æ¥è¿‘ç›®æ ‡é•¿åº¦ï¼Œè€ƒè™‘æ˜¯å¦ç»“æŸå½“å‰å—
            if len(sentence) > target_size * 0.3:  # å¦‚æœä¸‹ä¸€å¥å¾ˆé•¿ï¼Œç»“æŸå½“å‰å—
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += " " + sentence
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    # å¤„ç†æœ€åä¸€ä¸ªå—
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def print_chunk_analysis(chunks, method_name):
    """æ‰“å°åˆ‡ç‰‡åˆ†æç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {method_name}")
    print(f"{'='*60}")
    
    if not chunks:
        print("âŒ æœªç”Ÿæˆä»»ä½•åˆ‡ç‰‡")
        return
    
    total_length = sum(len(chunk) for chunk in chunks)
    avg_length = total_length / len(chunks)
    min_length = min(len(chunk) for chunk in chunks)
    max_length = max(len(chunk) for chunk in chunks)
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - åˆ‡ç‰‡æ•°é‡: {len(chunks)}")
    print(f"   - å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"   - æœ€çŸ­é•¿åº¦: {min_length} å­—ç¬¦")
    print(f"   - æœ€é•¿é•¿åº¦: {max_length} å­—ç¬¦")
    print(f"   - é•¿åº¦æ–¹å·®: {max_length - min_length} å­—ç¬¦")
    
    print(f"\nğŸ“ åˆ‡ç‰‡å†…å®¹:")
    for i, chunk in enumerate(chunks, 1):
        print(f"   å— {i} ({len(chunk)} å­—ç¬¦):")
        print(f"   {chunk}")
        print()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # æµ‹è¯•æ–‡æœ¬
    text = """
è¿ªå£«å°¼ä¹å›­æä¾›å¤šç§é—¨ç¥¨ç±»å‹ä»¥æ»¡è¶³ä¸åŒæ¸¸å®¢éœ€æ±‚ã€‚ä¸€æ—¥ç¥¨æ˜¯æœ€åŸºç¡€çš„é—¨ç¥¨ç±»å‹ï¼Œå¯åœ¨è´­ä¹°æ—¶é€‰å®šæ—¥æœŸä½¿ç”¨ï¼Œä»·æ ¼æ ¹æ®å­£èŠ‚æµ®åŠ¨ã€‚ä¸¤æ—¥ç¥¨éœ€è¦è¿ç»­ä¸¤å¤©ä½¿ç”¨ï¼Œæ€»ä»·æ¯”è´­ä¹°ä¸¤å¤©å•æ—¥ç¥¨ä¼˜æƒ çº¦9æŠ˜ã€‚ç‰¹å®šæ—¥ç¥¨åŒ…å«éƒ¨åˆ†èŠ‚åº†æ´»åŠ¨æ—¶æ®µï¼Œéœ€æ³¨æ„é—¨ç¥¨æ ‡æ³¨çš„æœ‰æ•ˆæœŸé™ã€‚

è´­ç¥¨æ¸ é“ä»¥å®˜æ–¹æ¸ é“ä¸ºä¸»ï¼ŒåŒ…æ‹¬ä¸Šæµ·è¿ªå£«å°¼å®˜ç½‘ã€å®˜æ–¹Appã€å¾®ä¿¡å…¬ä¼—å·åŠå°ç¨‹åºã€‚ç¬¬ä¸‰æ–¹å¹³å°å¦‚é£çŒªã€æºç¨‹ç­‰åˆä½œä»£ç†å•†ä¹Ÿå¯è´­ç¥¨ï¼Œä½†éœ€è®¤å‡†å®˜æ–¹æˆæƒæ ‡è¯†ã€‚æ‰€æœ‰ç”µå­ç¥¨éœ€ç»‘å®šèº«ä»½è¯ä»¶ï¼Œæ¸¯æ¾³å°å±…æ°‘å¯ç”¨é€šè¡Œè¯ï¼Œå¤–ç±æ¸¸å®¢ç”¨æŠ¤ç…§ï¼Œå„¿ç«¥ç¥¨éœ€æä¾›å‡ºç”Ÿè¯æ˜æˆ–æˆ·å£æœ¬å¤å°ä»¶ã€‚

ç”Ÿæ—¥ç¦åˆ©éœ€åœ¨å®˜æ–¹æ¸ é“ç™»è®°ï¼Œå¯è·èµ ç”Ÿæ—¥å¾½ç« å’Œç”œå“åˆ¸ã€‚åŠå¹´å†…æœ‰æ•ˆç»“å©šè¯æŒæœ‰è€…å¯è´­ä¹°ç‰¹åˆ«å¥—ç¥¨ï¼Œå«çš‡å®¶å®´ä¼šå…åŒäººé¤ã€‚å†›äººä¼˜æƒ ç°å½¹åŠé€€å½¹å†›äººå‡­è¯ä»¶äº«8æŠ˜ï¼Œéœ€è‡³å°‘æå‰3å¤©ç™»è®°å®¡æ‰¹ã€‚
"""
    
    print("ğŸ¯ åˆ‡ç‰‡ç­–ç•¥å¯¹æ¯”æµ‹è¯•")
    print(f"ğŸ“„ æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    
    # æµ‹è¯•å‚æ•°
    target_size = 300
    
    # 1. å›ºå®šé•¿åº¦åˆ‡ç‰‡
    chunks1 = improved_fixed_length_chunking(text, chunk_size=target_size, overlap=50)
    print_chunk_analysis(chunks1, "1. å›ºå®šé•¿åº¦åˆ‡ç‰‡")
    
    # 2. å¥å­è¾¹ç•Œåˆ‡ç‰‡
    chunks2 = semantic_chunking(text, max_chunk_size=target_size)
    print_chunk_analysis(chunks2, "2. å¥å­è¾¹ç•Œåˆ‡ç‰‡")
    
    # 3. LLMè¯­ä¹‰åˆ‡ç‰‡ï¼ˆLLMï¼‰
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ‡ç‰‡...")
    chunks3 = advanced_semantic_chunking_with_llm(text, max_chunk_size=target_size)
    print_chunk_analysis(chunks3, "3. LLMè¯­ä¹‰åˆ‡ç‰‡ï¼ˆLLMï¼‰")
    
    # 4. æ»‘åŠ¨çª—å£åˆ‡ç‰‡
    chunks4 = sliding_window_chunking(text, window_size=target_size, step_size=target_size//2)
    print_chunk_analysis(chunks4, "4. æ»‘åŠ¨çª—å£åˆ‡ç‰‡")
    
    # 5. è‡ªé€‚åº”åˆ‡ç‰‡
    chunks5 = adaptive_chunking(text, target_size=target_size, tolerance=0.3)
    print_chunk_analysis(chunks5, "5. è‡ªé€‚åº”åˆ‡ç‰‡")
    
    # 6. æ™ºèƒ½è‡ªé€‚åº”åˆ‡ç‰‡
    chunks6 = smart_adaptive_chunking(text, target_size=target_size, min_size=100, max_size=500)
    print_chunk_analysis(chunks6, "6. æ™ºèƒ½è‡ªé€‚åº”åˆ‡ç‰‡")
    
    # æ€»ç»“å¯¹æ¯”
    print(f"\n{'='*80}")
    print("ğŸ“ˆ ç­–ç•¥å¯¹æ¯”æ€»ç»“")
    print(f"{'='*80}")
    
    methods = [
        ("å›ºå®šé•¿åº¦", chunks1),
        ("å¥å­è¾¹ç•Œåˆ‡ç‰‡", chunks2),
        ("LLMè¯­ä¹‰åˆ‡ç‰‡", chunks3),
        ("æ»‘åŠ¨çª—å£", chunks4),
        ("è‡ªé€‚åº”åˆ‡ç‰‡", chunks5),
        ("æ™ºèƒ½è‡ªé€‚åº”", chunks6)
    ]
    
    print(f"{'ç­–ç•¥':<12} {'åˆ‡ç‰‡æ•°':<6} {'å¹³å‡é•¿åº¦':<8} {'é•¿åº¦æ–¹å·®':<8} {'æ¨èåº¦':<8}")
    print("-" * 50)
    
    for name, chunks in methods:
        if chunks:
            avg_len = sum(len(c) for c in chunks) / len(chunks)
            min_len = min(len(c) for c in chunks)
            max_len = max(len(c) for c in chunks)
            variance = max_len - min_len
            
            # ç®€å•çš„æ¨èåº¦è¯„ä¼°
            if len(chunks) >= 2 and variance < 100 and avg_len > 150:
                recommendation = "â­â­â­â­â­"
            elif len(chunks) >= 2 and variance < 150:
                recommendation = "â­â­â­â­"
            elif len(chunks) >= 1:
                recommendation = "â­â­â­"
            else:
                recommendation = "â­â­"
            
            print(f"{name:<12} {len(chunks):<6} {avg_len:<8.1f} {variance:<8.1f} {recommendation:<8}")
        else:
            print(f"{name:<12} {'0':<6} {'N/A':<8} {'N/A':<8} {'â­':<8}")

if __name__ == "__main__":
    import json
    main() 