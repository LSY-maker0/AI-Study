# qwen_agent/searcher/elasticsearch_searcher.py
import os
import json
import hashlib
import logging
from elasticsearch import Elasticsearch, helpers
from elasticsearch.exceptions import BadRequestError  # å¯¼å…¥ç‰¹å®šçš„å¼‚å¸¸
from qwen_agent.tools.doc_parser import DocParser
from openai import OpenAI

# ä¸ºæ­¤æ¨¡å—è®¾ç½®ä¸€ä¸ªæ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

class ElasticsearchSearcher:
    """ä¸€ä¸ªä½¿ç”¨ Elasticsearch è¿›è¡Œæ–‡æ¡£ç´¢å¼•å’Œæœç´¢çš„æœç´¢å™¨ã€‚"""

    def __init__(self, cfg):
        self.cfg = cfg
        es_cfg = cfg.get('es', {})
        self.host = es_cfg.get('host', 'http://localhost')
        self.port = es_cfg.get('port', 9200)
        self.user = es_cfg.get('user')
        self.password = es_cfg.get('password')
        self.index_name = es_cfg.get('index_name', 'qwen_agent_rag_idx')
        self.search_type = es_cfg.get('search_type', 'keywords')
        self.embedding_client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        # DocParser ç”¨äºè§£æå’Œåˆ†å—æ–‡æ¡£
        self.parser = DocParser(cfg=self.cfg)

        self.client = self._connect()
        if self.client:
            logger.info("æˆåŠŸè¿æ¥åˆ° Elasticsearchï¼")
            self._create_index_if_not_exists()
        else:
            logger.error("è¿æ¥ Elasticsearch å¤±è´¥ã€‚è¯·æ£€æŸ¥æ‚¨çš„é…ç½®ã€ç½‘ç»œå’Œ ES æœåŠ¡çŠ¶æ€ã€‚")

    def _connect(self) -> Elasticsearch:
        """å»ºç«‹å¹¶è¿”å›åˆ° Elasticsearch çš„è¿æ¥ã€‚"""
        try:
            # æ ¹æ®æä¾›çš„é…ç½®æ„å»ºè¿æ¥å‚æ•°
            es_args = {
                'hosts': [{
                    'host': self.host.replace('https://', '').replace('http://', ''),
                    'port': self.port,
                    'scheme': 'https' if 'https' in self.host else 'http',
                }],
                'verify_certs': False,  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è®¾ä¸º True å¹¶æä¾›è¯ä¹¦
                'request_timeout': 60,
            }
            if self.user and self.password:
                es_args['basic_auth'] = (self.user, self.password)

            client = Elasticsearch(**es_args)

            # æ£€æŸ¥è¿æ¥
            if not client.ping():
                raise ConnectionError("Elasticsearch ping å¤±è´¥ã€‚")

            return client
        except Exception as e:
            logger.error(f"æ— æ³•è¿æ¥åˆ° Elasticsearchï¼š{e}")
            return None

    def _create_index_if_not_exists(self):
        """
        å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒã€‚
        ä¼˜å…ˆå°è¯•ä½¿ç”¨ IK ä¸­æ–‡åˆ†è¯å™¨ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ ‡å‡†åˆ†è¯å™¨ã€‚
        """
        try:
            if not self.client.indices.exists(index=self.index_name):
                logger.info(f"ç´¢å¼• '{self.index_name}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")

                # ä¼˜å…ˆå°è¯•ä½¿ç”¨ IK åˆ†è¯å™¨çš„é…ç½®
                ik_index_settings = {
                    "settings": {"analysis": {"analyzer": {"default": {"type": "ik_max_word"}}}},
                    "mappings": {
                        "properties": {
                            "content": {"type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_smart"},
                            "source": {"type": "keyword"},
                            "token": {"type": "integer"},
                            # æ·»åŠ å‘é‡å­—æ®µ
                            "vector_content": {
                                "type": "dense_vector",
                                "dims": 1024,  # æ ¹æ®ä½ ç”¨çš„æ¨¡å‹ç»´åº¦è°ƒæ•´
                                "index": True,
                                "similarity": "cosine"
                            }
                        }
                    }
                }

                try:
                    # é¦–æ¬¡å°è¯•ä½¿ç”¨ IK åˆ›å»º
                    self.client.indices.create(index=self.index_name, body=ik_index_settings)
                    logger.info(f"æˆåŠŸä½¿ç”¨ IK åˆ†è¯å™¨åˆ›å»ºç´¢å¼• '{self.index_name}'ã€‚")
                except BadRequestError as e:
                    # æ•è·å› åˆ†è¯å™¨ä¸å­˜åœ¨å¯¼è‡´çš„é”™è¯¯
                    if 'Unknown analyzer type [ik_max_word]' in str(e):
                        logger.warning(
                            "æœªèƒ½æ‰¾åˆ° 'ik_max_word' åˆ†è¯å™¨ã€‚è¿™é€šå¸¸æ˜¯å› ä¸º Elasticsearch æœªå®‰è£… IK ä¸­æ–‡åˆ†è¯æ’ä»¶ã€‚")
                        logger.warning("å°†å›é€€ä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨ã€‚å¯¹äºä¸­æ–‡æœç´¢ï¼Œå¼ºçƒˆå»ºè®®å®‰è£… IK æ’ä»¶ä»¥è·å¾—æ›´å¥½æ•ˆæœã€‚")

                        # å›é€€é…ç½®ï¼šä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨
                        standard_index_settings = {
                            "mappings": {
                                "properties": {
                                    "content": {"type": "text"},  # ä½¿ç”¨é»˜è®¤çš„æ ‡å‡†åˆ†è¯å™¨
                                    "source": {"type": "keyword"},
                                    # æ·»åŠ å‘é‡å­—æ®µ
                                    "vector_content": {
                                        "type": "dense_vector",
                                        "dims": 1024,  # æ ¹æ®ä½ ç”¨çš„æ¨¡å‹ç»´åº¦è°ƒæ•´
                                        "index": True,
                                        "similarity": "cosine"
                                    }
                                }
                            }
                        }
                        # å†æ¬¡å°è¯•ä½¿ç”¨æ ‡å‡†é…ç½®åˆ›å»º
                        self.client.indices.create(index=self.index_name, body=standard_index_settings)
                        logger.info(f"æˆåŠŸä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨åˆ›å»ºç´¢å¼• '{self.index_name}'ã€‚")
                    else:
                        # å¦‚æœæ˜¯å…¶ä»–ç±»å‹çš„è¯·æ±‚é”™è¯¯ï¼Œåˆ™é‡æ–°å¼•å‘å¼‚å¸¸
                        raise e
            else:
                logger.info(f"ç´¢å¼• '{self.index_name}' å·²å­˜åœ¨ã€‚")
        except Exception as e:
            logger.error(f"åˆ›å»ºæˆ–æ£€æŸ¥ç´¢å¼• '{self.index_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

    def _get_embedding(self, text: str) -> list:
        """ä½¿ç”¨ Dashscope çš„ text-embedding-v4 æ¨¡å‹ä¸ºæ–‡æœ¬ç”Ÿæˆå‘é‡ã€‚"""
        print('111111')
        try:
            # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
            if not text.strip():
                return []

            response = self.embedding_client.embeddings.create(
                model="text-embedding-v4",
                input=text,
                dimensions=1024,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"  - è·å– embedding æ—¶å‡ºé”™: {e}")
            return []

    def index_files(self, files: list):
        """
        é«˜æ•ˆåœ°ç´¢å¼•æ–‡ä»¶åˆ—è¡¨ã€‚
        å®ƒé¦–å…ˆè·å–æ‰€æœ‰æ–‡ä»¶çš„æ‰€æœ‰æ–‡æœ¬å—ï¼Œç„¶åé€šè¿‡ä¸€æ¬¡ mget è¯·æ±‚è¿‡æ»¤æ‰å·²å­˜åœ¨çš„å—ï¼Œ
        æœ€åé€šè¿‡ä¸€æ¬¡ bulk è¯·æ±‚æ‰¹é‡ç´¢å¼•æ‰€æœ‰æ–°å—ã€‚
        """
        if not self.client:
            logger.error("Elasticsearch å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œç´¢å¼•ã€‚")
            return

        logger.info(f"å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ä»¥è¿›è¡Œç´¢å¼•...")
        chunks = self._get_chunks(files)
        print(f"ä»æ–‡ä»¶ä¸­æ€»å…±æå–äº† {len(chunks)} ä¸ªå†…å®¹å—ã€‚")

        if not chunks:
            logger.warning("æœªèƒ½ä»æ–‡ä»¶ä¸­æå–ä»»ä½•å†…å®¹å—ï¼Œç´¢å¼•è¿‡ç¨‹ç»ˆæ­¢ã€‚")
            return

        print(' hiå•Šå¸ˆå¤§',chunks)

        # é«˜æ•ˆåœ°ç­›é€‰å‡ºéœ€è¦ç´¢å¼•çš„æ–°å—
        new_chunks = self._filter_existing_chunks_efficiently(chunks)

        for chunk in new_chunks:
            # TODO: æ¥å…¥ embedding æ¨¡å‹ç”Ÿæˆå‘é‡
            chunk['vector'] = self._get_embedding(chunk['content'])

        if new_chunks:
            logger.info(f'å‘ç° {len(new_chunks)} ä¸ªæ–°çš„æ–‡æ¡£å—ï¼Œå¼€å§‹å‘ Elasticsearch æ‰¹é‡ç´¢å¼•...')
            actions = [{
                "_op_type": "index",
                "_index": self.index_name,
                "_id": chunk['id'],
                "_source": {
                    "content": chunk['content'],
                    "source": chunk['metadata']['source'],
                    "token": chunk.get('token', 0),
                    "vector_content": chunk['vector']  # æ·»åŠ å‘é‡æ•°æ®
                },
            } for chunk in new_chunks]

            try:
                successes, errors = helpers.bulk(self.client, actions, refresh=True, raise_on_error=False)
                logger.info(f"æˆåŠŸç´¢å¼• {successes} ä¸ªæ–°æ–‡æ¡£å—ã€‚")
                if errors:
                    logger.error(f"æ‰¹é‡ç´¢å¼•è¿‡ç¨‹ä¸­å‘ç”Ÿ {len(errors)} ä¸ªé”™è¯¯ã€‚ç¬¬ä¸€ä¸ªé”™è¯¯è¯¦æƒ…: {errors[0]}")
            except helpers.BulkIndexError as e:
                logger.error(f"æ‰¹é‡ç´¢å¼•æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {len(e.errors)} ä¸ªæ–‡æ¡£ç´¢å¼•å¤±è´¥ã€‚")
        else:
            logger.info("æ‰€æœ‰æ–‡ä»¶å†…å®¹å‡å·²åœ¨ Elasticsearch ä¸­å»ºç«‹ç´¢å¼•ï¼Œæ— éœ€æ›´æ–°ã€‚")

    def _get_chunks(self, files: list) -> list:
        """ä»æ–‡ä»¶åˆ—è¡¨ä¸­æå–å¹¶è¿”å›æ‰€æœ‰æ–‡æœ¬å—ã€‚"""
        all_chunks = []
        for file_path in files:
            try:
                # 1. å‡†å¤‡ JSON å­—ç¬¦ä¸²å‚æ•°
                params_str = json.dumps({'url': file_path})

                # 2. è°ƒç”¨ DocParserï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ª JSON å­—ç¬¦ä¸²
                parsed_content_str = self.parser.call(
                    params=params_str,
                    use_cache=False  # å¼ºåˆ¶é‡æ–°è§£æï¼Œå¿½ç•¥ç¼“å­˜
                )

                # 3. è§£æè¿”å›çš„ JSON å­—ç¬¦ä¸²
                parsed_record = parsed_content_str

                # æ£€æŸ¥è§£æåçš„è®°å½•æ˜¯å¦å‡ºé”™
                if 'error' in parsed_record:
                    logger.error(f"è§£ææ–‡ä»¶ '{file_path}' æ—¶è¿”å›é”™è¯¯: {parsed_record['error']}")
                    continue

                # ä»è®°å½•ä¸­æå– 'raw' å—
                chunks_data = parsed_record.get('raw', [])

                # ä¸ºæ¯ä¸ªå—æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
                for chunk in chunks_data:
                    if 'metadata' in chunk and 'source' not in chunk['metadata']:
                        chunk['metadata']['source'] = os.path.basename(file_path)
                    all_chunks.append(chunk)

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}", exc_info=True)
        return all_chunks

    def _filter_existing_chunks_efficiently(self, chunks: list) -> list:
        """
        ä½¿ç”¨ mget é«˜æ•ˆåœ°ä»å—åˆ—è¡¨ä¸­ç­›é€‰å‡ºå°šæœªåœ¨ESä¸­ç´¢å¼•çš„å—ã€‚
        """
        if not chunks:
            return []

        # 1. ä¸ºæ‰€æœ‰å—ç”Ÿæˆ ID
        for chunk in chunks:
            chunk_content = chunk.get('content', '')
            chunk_source = chunk.get('source', 'unknown')
            sha256 = hashlib.sha256()
            sha256.update(chunk_content.encode('utf-8'))
            sha256.update(chunk_source.encode('utf-8'))
            chunk['id'] = sha256.hexdigest()

        doc_ids = [chunk['id'] for chunk in chunks]

        # 2. ä½¿ç”¨ mget ä¸€æ¬¡æ€§æ£€æŸ¥æ‰€æœ‰ ID æ˜¯å¦å­˜åœ¨
        try:
            response = self.client.mget(index=self.index_name, body={'ids': doc_ids})
            existing_ids = {doc['_id'] for doc in response['docs'] if doc['found']}
            logger.info(f"åœ¨ Elasticsearch ä¸­å‘ç° {len(existing_ids)} ä¸ªå·²å­˜åœ¨çš„æ–‡æ¡£å—ã€‚")
        except Exception as e:
            logger.error(f"ä½¿ç”¨ mget æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}ã€‚å°†å‡å®šæ‰€æœ‰å—éƒ½æ˜¯æ–°çš„ã€‚")
            existing_ids = set()

        # 3. ç­›é€‰å‡ºæ–°å—
        new_chunks = [chunk for chunk in chunks if chunk['id'] not in existing_ids]
        # new_chunks = chunks
        logger.info(f"ç­›é€‰å‡º {len(new_chunks)} ä¸ªæ–°å—éœ€è¦ç´¢å¼•ã€‚")
        return new_chunks

    def print_hits(self,query,hits):
        # ===== ğŸ–¨ï¸ æ‰“å°æ‰€æœ‰æ£€ç´¢ç»“æœ =====
        print("\n" + "=" * 80)
        print(f"ğŸ” [ES æœç´¢å™¨] æŸ¥è¯¢è¯: '{query}'")
        print(f"ğŸ“Š å‘½ä¸­æ€»æ•°: {len(hits)}")
        print("=" * 80)

        for idx, item in enumerate(hits, 1):
            source = item['_source']

            # æå–å„ä¸ªå­—æ®µ
            content = source.get('content', '')
            source_file = source.get('source', 'æœªçŸ¥æ¥æº')
            token_count = source.get('token', 0)
            score = item.get('_score', 0)

            content_full = content[:100] if content else "(æ— å†…å®¹)"

            print(f"\n  ç»“æœ #{idx}")
            print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            print(f"  â”‚ Score:  {score:.4f}")
            print(f"  â”‚ Token:   {token_count}")
            print(f"  â”‚ æ¥æº:    {source_file}")
            print(f"  â”‚ Content: \n{content_full}")  # è¿™é‡Œæ‰“å°å…¨éƒ¨å†…å®¹
            print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

        print("=" * 80 + "\n")
        # ================================

    def search(self, query: str, max_ref_token: int) -> list:
        """
        åœ¨ Elasticsearch ä¸­æ‰§è¡Œæœç´¢ï¼Œæ ¹æ® search_type é€‰æ‹©ä¸åŒçš„æŸ¥è¯¢æ–¹å¼ã€‚
        """
        if not self.client:
            logger.error("âŒ Elasticsearch å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæœç´¢ã€‚")
            return []

        logger.info(f"ğŸ” æ­£åœ¨ä½¿ç”¨æŸ¥è¯¢è¯­å¥åœ¨ Elasticsearch ä¸­æœç´¢: '{query}'ï¼Œæ£€ç´¢æ¨¡å¼: {self.search_type}")

        hits = []

        if self.search_type == 'keyword':
            # ========== æ¨¡å¼1: çº¯å…³é”®è¯æœç´¢ ==========
            logger.info(f"ğŸ”¤ ä½¿ç”¨ã€å…³é”®è¯æœç´¢ã€‘æ¨¡å¼...")

            search_body = {
                "query": {
                    "match": {
                        "content": query
                    }
                },
                "size": 6
            }

            try:
                response = self.client.search(index=self.index_name, body=search_body)
                hits = response['hits']['hits']
            except Exception as e:
                logger.error(f"âŒ Elasticsearch æœç´¢å¤±è´¥: {e}")
                return []

        elif self.search_type == 'vector':
            # ========== æ¨¡å¼2: çº¯å‘é‡æœç´¢ ==========
            logger.info(f"ğŸ§  ä½¿ç”¨ã€è¯­ä¹‰å‘é‡æœç´¢ã€‘æ¨¡å¼...")

            # ä¸ºæŸ¥è¯¢ç”Ÿæˆå‘é‡
            query_vector = self._get_embedding(query)
            if not query_vector:
                logger.warning("âš ï¸ æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢ã€‚")
                # å›é€€åˆ°å…³é”®è¯æœç´¢
                search_body = {
                    "query": {
                        "match": {
                            "content": query
                        }
                    },
                    "size": 6
                }
                response = self.client.search(index=self.index_name, body=search_body)
                hits = response['hits']['hits']
            else:
                search_body = {
                    "knn": {
                        "field": "vector_content",  # æŒ‡å®šå­—æ®µå
                        "query_vector": query_vector,  # æŒ‡å®šå‘é‡
                        "k": 6,
                        "num_candidates": 1000
                    },
                    "size": 6
                }

                try:
                    response = self.client.search(index=self.index_name, body=search_body)
                    hits = response['hits']['hits']
                except Exception as e:
                    logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢ã€‚")
                    # å›é€€åˆ°å…³é”®è¯æœç´¢
                    search_body = {
                        "query": {
                            "match": {
                                "content": query
                            }
                        },
                        "size": 6
                    }
                    response = self.client.search(index=self.index_name, body=search_body)
                    hits = response['hits']['hits']


        elif self.search_type == 'hybrid':

            # ========== æ¨¡å¼3: æ··åˆæœç´¢ï¼ˆæ‰‹åŠ¨ RRF ç‰ˆï¼‰å¸¦è¯¦ç»†è¯„åˆ† ==========

            logger.info(f"ğŸ”€ ä½¿ç”¨ã€æ··åˆæœç´¢ï¼ˆæ‰‹åŠ¨ RRFï¼‰ã€‘æ¨¡å¼...")

            query_vector = self._get_embedding(query)

            # å¦‚æœå‘é‡ç”Ÿæˆå¤±è´¥ï¼Œçº¯å›é€€
            if not query_vector:
                logger.warning("âš ï¸ æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢ã€‚")
                search_body = {"query": {"match": {"content": query}}, "size": 6}
                response = self.client.search(index=self.index_name, body=search_body)
                hits = response['hits']['hits']
            else:
                # --- ç¬¬ä¸€æ­¥ï¼šå•ç‹¬è·‘å…³é”®è¯æœç´¢ ---
                bm25_search_body = {"query": {"match": {"content": query}}, "size": 6}
                try:
                    bm25_response = self.client.search(index=self.index_name, body=bm25_search_body)
                    bm25_hits = bm25_response['hits']['hits']
                except Exception as e:
                    logger.error(f"âŒ å…³é”®è¯å­æŸ¥è¯¢å¤±è´¥: {e}")
                    bm25_hits = []

                # --- ç¬¬äºŒæ­¥ï¼šå•ç‹¬è·‘å‘é‡æœç´¢ ---
                knn_search_body = {
                    "knn": {
                        "field": "vector_content",
                        "query_vector": query_vector,
                        "k": 6,
                        "num_candidates": 1000
                    },
                    "size": 6
                }
                try:
                    knn_response = self.client.search(index=self.index_name, body=knn_search_body)
                    knn_hits = knn_response['hits']['hits']
                except Exception as e:
                    logger.error(f"âŒ å‘é‡å­æŸ¥è¯¢å¤±è´¥: {e}")
                    knn_hits = []
                # --- ç¬¬ä¸‰æ­¥ï¼šæ‰‹åŠ¨æ‰§è¡ŒåŠ æƒ RRF (Reciprocal Rank Fusion) ---

                # 1. å¹³è¡¡æ¨¡å¼ (æ¨è)ï¼šå…³é”®è¯å’Œå‘é‡äº”äº”å¼€ï¼Œå…¼é¡¾ç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰ç†è§£
                MODE_BALANCED = {"k": 60, "bm25": 0.5, "knn": 0.5}

                # 2. åå‘é‡æ¨¡å¼ï¼šä¼˜å…ˆè¯­ä¹‰ç›¸ä¼¼ï¼Œé€‚åˆæ¨¡ç³ŠæŸ¥è¯¢æˆ–æ‰¾â€œç›¸å…³å†…å®¹â€
                MODE_TOWARD_VECTOR = {"k": 20, "bm25": 0.3, "knn": 0.7}

                # 3. åå…³é”®è¯æ¨¡å¼ï¼šä¼˜å…ˆç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆæœäººåã€ä»£ç ã€ä¸“æœ‰åè¯
                MODE_TOWARD_KEYWORD = {"k": 100, "bm25": 0.7, "knn": 0.3}

                # ğŸ‘‡ğŸ‘‡ğŸ‘‡ ã€åœ¨è¿™é‡Œé€‰æ‹©æ¨¡å¼ã€‘ ğŸ‘‡ğŸ‘‡ğŸ‘‡
                current_mode = MODE_TOWARD_VECTOR  # ä¿®æ”¹è¿™é‡Œåˆ‡æ¢æ¨¡å¼ï¼šMODE_BALANCED / MODE_TOWARD_VECTOR / MODE_TOWARD_KEYWORD

                # æå–å‚æ•°
                k_constant = current_mode["k"]
                bm25_weight = current_mode["bm25"]
                knn_weight = current_mode["knn"]

                logger.info(f"âš™ï¸  å½“å‰æ··åˆæœç´¢æ¨¡å¼é…ç½®: K={k_constant}, BM25æƒé‡={bm25_weight}, KNNæƒé‡={knn_weight}")

                rrf_scores = {}

                # ğŸ‘‡ æ‰“å°å¹¶è®¡ç®—å…³é”®è¯è¯„åˆ†
                print("\n" + "=" * 30 + " ğŸ“Š å…³é”®è¯è¯„åˆ† (BM25) " + "=" * 30)
                for rank, hit in enumerate(bm25_hits):
                    doc_id = hit['_id']
                    content_preview = hit['_source'].get('content', '')[:15].replace('\n', ' ')

                    # âœ… ä¿®æ”¹ç‚¹1ï¼šåº”ç”¨æƒé‡ç³»æ•° (weight / (k + rank))
                    score = (bm25_weight / (k_constant + rank + 1))
                    rrf_scores[doc_id] = {"score": score, "hit": hit}

                    print(
                        f"  Rank {rank}: ID:{doc_id[:6]}... | '{content_preview}...' | (+{score:.4f} åˆ†) [æƒé‡:{bm25_weight}]")

                # ğŸ‘‡ æ‰“å°å¹¶è®¡ç®—å‘é‡è¯„åˆ†
                print("\n" + "=" * 30 + " ğŸ§  å‘é‡è¯„åˆ† (KNN) " + "=" * 30)
                for rank, hit in enumerate(knn_hits):
                    doc_id = hit['_id']
                    content_preview = hit['_source'].get('content', '')[:15].replace('\n', ' ')

                    # âœ… ä¿®æ”¹ç‚¹2ï¼šåº”ç”¨æƒé‡ç³»æ•° (weight / (k + rank))
                    score = (knn_weight / (k_constant + rank + 1))

                    if doc_id in rrf_scores:
                        # åŒæ–™å† å†›ï¼šåˆ†æ•°ç´¯åŠ 
                        rrf_scores[doc_id]["score"] += score
                        print(
                            f"  Rank {rank}: ID:{doc_id[:6]}... | '{content_preview}...' | (+{score:.4f} åˆ†) â­ åŒæ–™å† å†›ï¼æ€»åˆ†: {rrf_scores[doc_id]['score']:.4f}")
                    else:
                        # å•æ‰“ç‹¬æ–—
                        rrf_scores[doc_id] = {"score": score, "hit": hit}
                        print(
                            f"  Rank {rank}: ID:{doc_id[:6]}... | '{content_preview}...' | (+{score:.4f} åˆ†) [æƒé‡:{knn_weight}]")

                # --- ç¬¬å››æ­¥ï¼šæŒ‰ RRF å¾—åˆ†æ’åºå¹¶å–å‰ 6 ---
                sorted_hits = sorted(
                    rrf_scores.values(),
                    key=lambda x: x["score"],
                    reverse=True
                )

                # ğŸ‘‡ æ‰“å°æœ€ç»ˆèåˆæ’è¡Œæ¦œ
                print("\n" + "=" * 30 + " ğŸ† æœ€ç»ˆèåˆæ¦œå• (Top 6) " + "=" * 30)
                hits = []
                for i, item in enumerate(sorted_hits[:6]):
                    hit = item["hit"]
                    score = item["score"]
                    hit['_score'] = score  # æ›´æ–° _score ä¾›åç»­ä½¿ç”¨
                    hits.append(hit)

                    content_preview = hit['_source'].get('content', '')[:15].replace('\n', ' ')
                    print(f"  No.{i + 1} | ID:{hit['_id'][:6]}... | '{content_preview}...' | æ€»åˆ†: {score:.4f}")

                print("=" * 80 + "\n")

                logger.info(f"ğŸ§© RRF èåˆå®Œæˆï¼šBM25 {len(bm25_hits)} æ¡ + KNN {len(knn_hits)} æ¡ -> æœ€ç»ˆ {len(hits)} æ¡")


        else:
            # æœªçŸ¥çš„æ£€ç´¢æ¨¡å¼ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢
            logger.warning(f"âš ï¸ æœªçŸ¥çš„æ£€ç´¢æ¨¡å¼: {self.search_type}ï¼Œä½¿ç”¨é»˜è®¤çš„å…³é”®è¯æœç´¢ã€‚")
            search_body = {
                "query": {
                    "match": {
                        "content": query
                    }
                },
                "size": 6
            }
            response = self.client.search(index=self.index_name, body=search_body)
            hits = response['hits']['hits']

        # æ‰“å°ç»“æœ
        self.print_hits(query, hits)

        # æ ¹æ® max_ref_token ç­›é€‰ç»“æœ
        selected_hits = []
        total_tokens = 0
        for hit in hits:
            token_count = hit['_source'].get('token', 1000)
            if total_tokens + token_count > max_ref_token:
                logger.info(f'â±ï¸  å·²è¾¾åˆ° max_ref_token ({max_ref_token}) çš„ä¸Šé™ï¼Œåœæ­¢æ·»åŠ æ›´å¤šç»“æœã€‚')
                break
            selected_hits.append(hit)
            total_tokens += token_count

        logger.info(
            f"âœ… æœç´¢å®Œæˆï¼Œä» {len(hits)} ä¸ªå€™é€‰ä¸­ç­›é€‰å‡º {len(selected_hits)} ä¸ªç»“æœ (æ€»è®¡ {total_tokens} tokens)ã€‚")
        return selected_hits